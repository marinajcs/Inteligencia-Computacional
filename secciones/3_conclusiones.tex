\section{Conclusiones}

A lo largo del desarrollo de esta práctica, se implementaron y analizaron diversas arquitecturas de redes neuronales, cada una con un nivel de complejidad creciente, adaptadas al reconocimiento óptico de caracteres en el conjunto de datos MNIST. Estas arquitecturas incluyen redes neuronales simples, multicapa, convolucionales y modelos combinados con autoencoders, culminando en un refinamiento avanzado de una CNN mejorada. 

La primera red, una arquitectura básica sin capas ocultas, presentó limitaciones evidentes en su capacidad de generalización debido a su simplicidad, alcanzando un error de prueba del 7.39\%. La introducción de una capa oculta en la segunda red mejoró significativamente el rendimiento, reduciendo el error al 1.78\%, demostrando la importancia de añadir capacidad de representación.

Con el avance hacia las redes convolucionales, se logró una notable mejora en la precisión, con la tercera red (CNN básica, NN-3) alcanzando un error de prueba de 0.97\%. Este modelo demostró la efectividad de las capas convolutivas y de pooling para capturar patrones espaciales relevantes en los datos. Posteriormente, el uso de autoencoders en la cuarta red (NN-4) permitió combinar extracción de características con aprendizaje supervisado, aunque su error de 2.10\% reflejó que este enfoque es más adecuado para problemas de reducción de dimensionalidad.

Después, se vieron enfoques combinados basados en los cuatro primeros modelos. La red neuronal convolucional mejorada (NN-5), que integró estrategias avanzadas como aumento de datos, dropout y normalización por lotes, logró un error de prueba de 0.38\%. Acto seguido, se intentó abordar el problema combinando la NN-4 y NN-5 (NN-6), aunque este enfoque no consiguió mejorar los resultados de la NN-5. 

Finalmente, se decidió llevar a cabo el refinamiento de la CNN mejorada, lo que dio lugar a la creación de la NN-7. Mediante ajustes en la arquitectura y optimización, se logró reducir el error aún más, alcanzando un 0.28\% de error sobre el conjunto de prueba, superando así el objetivo inicial.

En conclusión, este proyecto demuestra cómo la integración progresiva de técnicas avanzadas de deep learning mejora significativamente el rendimiento en tareas de clasificación. Los resultados subrayan la importancia de estrategias como la normalización, regularización y aumento de datos para maximizar la generalización y minimizar el sobreajuste en redes profundas. La NN-7 representa un balance entre complejidad y rendimiento, consolidándose como el modelo más efectivo (de los vistos) para el problema planteado.


